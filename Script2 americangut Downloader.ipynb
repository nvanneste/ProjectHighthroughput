{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "import urllib.request as ur\n",
    "import urllib.parse as par\n",
    "\n",
    "\n",
    "def main():\n",
    "    url= \"https://www.openhumans.org/api/public-data/?source=american_gut\"\n",
    "    html = ur.urlopen(url).read()\n",
    "    print(type(html))\n",
    "    data = json.loads(html.decode())\n",
    "    print(type(data))\n",
    "    with open ('testfile.json', mode='w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request as ur\n",
    "from pprint import pprint\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "\n",
    "list_terms = ['diabetes', 'alzheimers','autoimmune' ]\n",
    "\n",
    "url= \"https://www.openhumans.org/api/public-data/?source=american_gut\"\n",
    "site = ur.urlopen(url).read()\n",
    "data = json.loads(site.decode())\n",
    "\n",
    "outfile = open('AmericanGut.txt', mode = 'a')\n",
    "    \n",
    "#als de next niet niets is dan gaat de loop eeuwig verder zo itereer je over heel de site\n",
    "while data['next'] != None:\n",
    "    for i in range(0,len(data[\"results\"]),1):\n",
    "        #zoekt naar metadata.json files en gaat deze dan de gegevens ophalen\n",
    "        if 'metadata.json' in data[\"results\"][i][\"basename\"]:\n",
    "            username = data[\"results\"][i][\"user\"][\"name\"]\n",
    "            downloadurl = data[\"results\"][i][\"download_url\"]\n",
    "            basename = data[\"results\"][i][\"basename\"]\n",
    "            print_out = (\" username: {} \\n url: {} \\n basename: {} \\n\".format(username, downloadurl, basename))\n",
    "            \n",
    "            #open de json file en zoek daarin de bepaalde ziekten en rapporteer ze\n",
    "            link_data = (ur.urlopen(downloadurl).read())\n",
    "            link = json.loads(link_data.decode())\n",
    "            for terms in list_terms:\n",
    "                if link:\n",
    "                    print_out +=( \"\\t\" + terms+ ':' + link[terms])\n",
    "                else:\n",
    "                    print_out += ('Not found')\n",
    "            print_out += (\"\\n\")\n",
    "            outfile.write(print_out)\n",
    "            \n",
    "    #open de volgende url en laat deze draaien als er over de 100 range is gepasseerd zodat de 2de link kan gestart \n",
    "    #worden\n",
    "    url = data['next']\n",
    "    site = ur.urlopen(url).read()\n",
    "    data = json.loads(site.decode())\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Harvard project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request as ur\n",
    "from pprint import pprint\n",
    "from urllib.request import urlopen\n",
    "\n",
    "list_terms = ['Severe disease or rare genetic trait']\n",
    "\n",
    "url= \"https://www.openhumans.org/api/public-data/?source=pgp\"\n",
    "site = ur.urlopen(url).read()\n",
    "data = json.loads(site.decode())\n",
    "\n",
    "header = (\"username ; url ; basename ;\")\n",
    "\n",
    "output_file = open('Harvard.csv', mode = 'a')\n",
    "\n",
    "for terms in list_terms:\n",
    "    header += (terms + \";\")\n",
    "output_file.write(header + \"\\n\")\n",
    "\n",
    "#als de next niet niets is dan gaat de loop eeuwig verder zo itereer je over heel de site\n",
    "while data['next'] != None:\n",
    "    #limit is lengte van de data+ sprongen van 1 om geen gegevens over te slaan\n",
    "    for i in range(0,len(data[\"results\"]),1):\n",
    "        #zoekt naar metadata.json files en gaat deze dan de gegevens ophalen\n",
    "        if 'surveys' and '.json' in data[\"results\"][i][\"basename\"]:\n",
    "            username = data[\"results\"][i][\"user\"][\"name\"]\n",
    "            downloadurl = data[\"results\"][i][\"download_url\"]\n",
    "            basename = data[\"results\"][i][\"basename\"]\n",
    "            print_out = (\" {};{};{};\".format(username, downloadurl, basename))\n",
    "            \n",
    "            #open de json file en zoek daarin de bepaalde ziekten en rapporteer ze\n",
    "            link_data = (ur.urlopen(downloadurl).read())\n",
    "            link = json.loads(link_data.decode())\n",
    "            for j in range(0, len(link)):\n",
    "                responses = link[j][\"responses\"]\n",
    "                for response in responses:\n",
    "                    nextquery = response[\"query\"]\n",
    "                    responsevalue = response[\"response\"]\n",
    "                    if nextquery in list_terms:\n",
    "                        print_out += (responsevalue + \"|\")\n",
    "            print_out += (\"\\n\")\n",
    "            output_file.write(print_out)\n",
    "            \n",
    "    #open de volgende url en laat deze draaien als er over de 100 range is gepasseerd zodat de 2de link kan gestart \n",
    "    #worden\n",
    "    url = data['next']\n",
    "    site = ur.urlopen(url).read()\n",
    "    data = json.loads(site.decode())\n",
    "\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request as ur\n",
    "from pprint import pprint\n",
    "from urllib.request import urlopen\n",
    "\n",
    "#alles wat ik wil opvragen + de mogelijke antwoorden\n",
    "list_terms = ['diabetes', 'alzheimers','autoimmune','chickenpox', 'age_years', 'weight_kg' ]\n",
    "neg_answers = ['I do not have this condition', 'No']\n",
    "dnk_answers = ['Unknown', 'Unspecified']\n",
    "pos_answers = ['yes', 'Yes']\n",
    "\n",
    "url= \"https://www.openhumans.org/api/public-data/?source=american_gut\"\n",
    "site = ur.urlopen(url).read()\n",
    "data = json.loads(site.decode())\n",
    "\n",
    "outfile = open('AmericanGut.csv', mode = 'a')\n",
    "\n",
    "#header info opmaken username, url,... ter navigatie\n",
    "header = (\"username ; url ; basename ;\")\n",
    "\n",
    "#laten loopen zodat er een bepaalde header iedere keer terug komt en de info er rechtstreeks onder komt\n",
    "for terms in list_terms:\n",
    "    header += (terms + \";\")\n",
    "outfile.write(header + \"\\n\")\n",
    "    \n",
    "#als de next niet niets is dan gaat de loop eeuwig verder zo itereer je over heel de site\n",
    "while data['next'] != None:\n",
    "    for i in range(0,len(data[\"results\"]),1):\n",
    "        #zoekt naar metadata.json files en gaat deze dan de gegevens ophalen\n",
    "        if 'metadata.json' in data[\"results\"][i][\"basename\"]:\n",
    "            username = data[\"results\"][i][\"user\"][\"name\"]\n",
    "            downloadurl = data[\"results\"][i][\"download_url\"]\n",
    "            basename = data[\"results\"][i][\"basename\"]\n",
    "            print_out = (\"{}; {}; {};\".format(username, downloadurl, basename))\n",
    "            \n",
    "            #open de json file en zoek daarin de bepaalde ziekten en rapporteer ze\n",
    "            link_data = (ur.urlopen(downloadurl).read())\n",
    "            link = json.loads(link_data.decode())\n",
    "            for terms in list_terms:\n",
    "                if link: #in de downloadjson kijken naar de antwoorden van de vermelde ziekten en omvormen naar binair\n",
    "                    ans = link[terms]\n",
    "                    if ans in neg_answers:\n",
    "                        print_out +=(\"0 ;\")\n",
    "                    elif ans in dnk_answers:\n",
    "                        print_out += (\"NA ;\")\n",
    "                    elif ans in pos_answers:\n",
    "                        print_out += (\"1 ;\")\n",
    "                    elif link:\n",
    "                        print_out += (ans + \";\")\n",
    "                else:\n",
    "                    print_out += (\"Not Found ;\")\n",
    "            print_out += (\"\\n\")\n",
    "            outfile.write(print_out)\n",
    "            \n",
    "    #open de volgende url en laat deze draaien als er over de len(data) range is gepasseerd zodat de volgende link \n",
    "    #kan gestart worden\n",
    "    url = data['next']\n",
    "    site = ur.urlopen(url).read()\n",
    "    data = json.loads(site.decode())\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
